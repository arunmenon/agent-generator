# src/agent_creator/crew.py

from typing import Any, Dict, List, Optional, Union
from pydantic import BaseModel, Field
from crewai import Agent, Crew, Process, Task, LLM
from crewai.project import CrewBase, agent, task, crew, before_kickoff

class CrewConfig(BaseModel):
    crew: Dict[str, Any]
    agents: Any
    tasks: Any
    input_schema_json: Any

# Structured data models for agent communication
class UserRequirements(BaseModel):
    """Structured user requirements extracted from input."""
    description: str = Field(..., description="High-level description of the project")
    input_description: str = Field(..., description="Description of input data sources")
    output_description: str = Field(..., description="Description of expected output format")
    tools: List[str] = Field(default_factory=list, description="List of tools to use")
    process: str = Field(default="sequential", description="Process type (sequential or hierarchical)")
    planning: bool = Field(default=False, description="Whether to use planning")
    knowledge: bool = Field(default=False, description="Whether to use knowledge")
    human_input_tasks: bool = Field(default=False, description="Whether to use human input tasks")
    memory: bool = Field(default=False, description="Whether to use memory")
    cache: bool = Field(default=False, description="Whether to use cache")
    manager_llm: Optional[str] = Field(default=None, description="Manager LLM to use")

class PlanConstraint(BaseModel):
    """Represents a constraint that plans must satisfy."""
    name: str = Field(..., description="Short identifier for the constraint")
    description: str = Field(..., description="Detailed explanation of the constraint")
    validation_prompt: str = Field(..., description="Prompt to check if a plan satisfies this constraint")

class ConstraintList(BaseModel):
    """List of constraints extracted from requirements."""
    constraints: List[PlanConstraint] = Field(default_factory=list)

class TaskDefinition(BaseModel):
    """Definition of a task in a plan."""
    name: str = Field(..., description="Name of the task")
    purpose: str = Field(..., description="Purpose or goal of the task")
    dependencies: List[str] = Field(default_factory=list, description="Tasks this task depends on")
    complexity: str = Field(default="Medium", description="Complexity level (Low, Medium, High)")

class AgentDefinition(BaseModel):
    """Definition of an agent in a plan."""
    name: str = Field(..., description="Name of the agent")
    role: str = Field(..., description="Role of the agent")
    goal: str = Field(..., description="Goal of the agent")
    backstory: str = Field(default="", description="Backstory of the agent")

class ConceptualPlan(BaseModel):
    """Conceptual plan with tasks and agent types."""
    planned_tasks: List[TaskDefinition] = Field(default_factory=list)
    required_agent_types: List[AgentDefinition] = Field(default_factory=list)

class Plan(BaseModel):
    """Represents a complete plan generated by an agent."""
    name: str = Field(..., description="Name of the plan")
    content: str = Field(..., description="Detailed description of the plan")
    planned_tasks: List[TaskDefinition] = Field(default_factory=list)
    required_agent_types: List[AgentDefinition] = Field(default_factory=list)
    score: Optional[float] = Field(default=None, description="Overall score of the plan")
    constraints_satisfied: List[str] = Field(default_factory=list, description="Constraints satisfied by this plan")
    constraints_violated: List[str] = Field(default_factory=list, description="Constraints violated by this plan")

class PlanEvaluation(BaseModel):
    """Evaluation of a plan against constraints."""
    constraints_satisfied: List[str] = Field(default_factory=list)
    constraints_violated: List[str] = Field(default_factory=list)
    completeness: int = Field(..., ge=1, le=10, description="Completeness score (1-10)")
    efficiency: int = Field(..., ge=1, le=10, description="Efficiency score (1-10)")
    feasibility: int = Field(..., ge=1, le=10, description="Feasibility score (1-10)")
    alignment: int = Field(..., ge=1, le=10, description="Alignment with requirements score (1-10)")
    explanations: Dict[str, str] = Field(default_factory=dict)
    total_score: int = Field(..., description="Sum of all scores")

class EvaluatedPlan(BaseModel):
    """Plan with its evaluation."""
    name: str = Field(..., description="Name of the plan")
    evaluation: PlanEvaluation = Field(...)

class PlanEvaluationResult(BaseModel):
    """Result of evaluating multiple plans."""
    evaluated_plans: List[EvaluatedPlan] = Field(...)
    best_plan: str = Field(..., description="Name of the highest scoring plan")

class AlternativePlans(BaseModel):
    """Collection of alternative plans."""
    plans: List[Plan] = Field(...)

class InputSchemaDefinition(BaseModel):
    """Definition of the input schema for the crew."""
    input_schema_json: Dict[str, Any] = Field(default_factory=dict)

class PlanGenResult(BaseModel):
    """Result from running multiple plan generation techniques."""
    best_plan: Plan = Field(...)
    all_plans: List[Plan] = Field(...)
    execution_details: Dict[str, Any] = Field(default_factory=dict)

@CrewBase
class MetaCrew():
    def __init__(self, llm_model="openai/gpt-4", n_samples=3, use_best_of_n=True, use_tot=False):
        self.llm_model = llm_model
        # Set verbose=False to reduce debug logs
        self.llm = LLM(model=self.llm_model, temperature=0.2, verbose=False)
        self.inputs: Dict[str, Any] = {}
        # PlanGEN parameters
        self.n_samples = n_samples  # Number of plans to generate for Best-of-N
        self.use_best_of_n = use_best_of_n  # Whether to use Best-of-N sampling
        self.use_tot = use_tot  # Whether to use Tree-of-Thought search

    @before_kickoff
    def capture_inputs(self, inputs: Dict[str, Any]):
        """
        Store user inputs in self.inputs so placeholders like {user_description}
        get replaced by CrewAI's .format(**inputs) at runtime.
        """
        self.inputs = inputs
        return inputs

    @agent
    def planner_agent(self) -> Agent:
        """
        Agent that proposes conceptual tasks & agent types from user requirements.
        """
        return Agent(
            role="Planning Architect",
            goal=(
                "Transform user requirements into a minimal, cohesive set of tasks "
                "and well-defined agent roles."
            ),
            backstory=(
                "A seasoned planning architect with expertise in orchestrating workflows, "
                "ensuring each task is minimal, actionable, and properly sequenced."
            ),
            llm=self.llm,
            memory=True,
            verbose=False,
            allow_delegation=False,
            max_iter=5,
            respect_context_window=True,
            use_system_prompt=True,
            cache=False,
            max_retry_limit=2,
        )
    
    @agent
    def constraint_agent(self) -> Agent:
        """
        Agent that identifies and validates constraints from user requirements.
        """
        return Agent(
            role="Constraint Analyst",
            goal=(
                "Identify, extract, and formalize constraints from user requirements that "
                "must be satisfied by any valid plan."
            ),
            backstory=(
                "An expert analyst specializing in identifying explicit and implicit constraints "
                "from requirements and formalizing them as validation checks."
            ),
            llm=self.llm,
            memory=True,
            verbose=False,
            allow_delegation=False,
            max_iter=3,
            respect_context_window=True,
            use_system_prompt=True,
            cache=False,
            max_retry_limit=2,
        )
    
    @agent
    def plan_evaluator_agent(self) -> Agent:
        """
        Agent that evaluates plans against constraints and provides scores.
        """
        return Agent(
            role="Plan Evaluator",
            goal=(
                "Systematically evaluate plans against all constraints and requirements, "
                "providing clear scores and detailed feedback."
            ),
            backstory=(
                "A rigorous evaluator with expertise in analyzing plans from multiple angles "
                "and providing objective assessments based on well-defined criteria."
            ),
            llm=self.llm,
            memory=True,
            verbose=False,
            allow_delegation=False,
            max_iter=3,
            respect_context_window=True,
            use_system_prompt=True,
            cache=False,
            max_retry_limit=2,
        )

    @agent
    def schema_converter(self) -> Agent:
        """
        Agent that merges tasks with input_schema_json and refines the final CrewAI schema.
        Preserves any quadruple-braced placeholders (e.g. {{{{title}}}}).
        """
        return Agent(
            role="Schema Converter",
            goal=(
                "Merge planned tasks/agents with a partial input_schema_json snippet into a final CrewAI config. "
                "Validate each agent/task, fill missing fields, and return the final JSON."
            ),
            backstory=(
                "A schema architect well-versed in CrewAI standards. Ensures tasks/agents are fully defined, "
                "removes extraneous info, and preserves placeholders if relevant."
            ),
            llm=self.llm,
            memory=True,
            verbose=False,
            allow_delegation=False,
            max_iter=5,
            respect_context_window=True,
            use_system_prompt=True,
            cache=False,
            max_retry_limit=2,
        )

    @task
    def gather_user_requirements_task(self) -> Task:
        """
        Gathers raw user inputs with placeholders. CrewAI .format(**inputs) will fill them
        before the LLM sees it. Returns a structured UserRequirements object.
        """
        description = """
Below are user inputs. Extract them into a structured format matching the UserRequirements model:

```python
class UserRequirements(BaseModel):
    description: str
    input_description: str
    output_description: str
    tools: List[str]
    process: str = "sequential"
    planning: bool = False
    knowledge: bool = False
    human_input_tasks: bool = False
    memory: bool = False
    cache: bool = False
    manager_llm: Optional[str] = None
```

Format your response as valid Python code that creates a UserRequirements instance. The user provided:

User Description: {user_description}
User Input Description: {user_input_description}
User Output Description: {user_output_description}
Tools: {user_tools}
Process: {user_process}
Planning: {user_planning}
Knowledge: {user_knowledge}
Human Input Tasks: {user_human_input_tasks}
Memory: {user_memory}
Cache: {user_cache}
Manager LLM: {user_manager_llm}

Your answer should start with "UserRequirements(" and end with ")" and include all fields.
"""
        return Task(
            description=description,
            expected_output="UserRequirements instance with structured user requirements",
            agent=self.planner_agent(),
            output_pydantic=UserRequirements
        )
        
    @task
    def identify_constraints_task(self) -> Task:
        """
        Identifies constraints from user requirements that any valid plan must satisfy.
        Returns a structured ConstraintList object.
        """
        description = r"""
Based on the user requirements:
{{output}}

Identify all constraints that any valid solution must satisfy. These can be explicit requirements 
or implicit constraints based on the problem domain.

Your response should match the following Pydantic models:

```python
class PlanConstraint(BaseModel):
    name: str
    description: str
    validation_prompt: str

class ConstraintList(BaseModel):
    constraints: List[PlanConstraint]
```

For each constraint:
1. Give it a clear name
2. Write a detailed description
3. Provide a validation prompt that could be used to check if a plan satisfies this constraint

Focus on identifying:
- Functional requirements (what the solution must do)
- Performance requirements (how well it must perform)
- Resource constraints (time, budget, tools, etc.)
- Domain-specific rules or limitations
- Dependencies between components

Format your response as Python code that creates a ConstraintList object containing all identified constraints.
Start with "ConstraintList(constraints=[" and include all PlanConstraint objects.
"""
        return Task(
            description=description,
            expected_output="ConstraintList object with identified constraints",
            agent=self.constraint_agent(),
            context=[self.gather_user_requirements_task()],
            output_pydantic=ConstraintList
        )

    @task
    def plan_tasks_and_agents_task(self) -> Task:
        """
        Proposes tasks & agent types using quadruple braces {{{{title}}}} if needed.
        Keep double braces for CrewAI placeholders (e.g. {{output}}).
        Returns a structured ConceptualPlan object.
        """
        # Raw string so Python doesn't treat backslashes/newlines specially.
        # Double braces to avoid KeyError from .format().
        description = r"""
Given these user requirements:
{{output}}

We also have 'inputDescription': {{{{user_input_description}}}}, which might imply placeholders
like {{{{title}}}}, {{{{targetLanguage}}}}, etc.

Your response should match the following Pydantic models:

```python
class TaskDefinition(BaseModel):
    name: str
    purpose: str
    dependencies: List[str] = []
    complexity: str = "Medium"  # Low, Medium, High

class AgentDefinition(BaseModel):
    name: str
    role: str
    goal: str
    backstory: str = ""

class ConceptualPlan(BaseModel):
    planned_tasks: List[TaskDefinition]
    required_agent_types: List[AgentDefinition]
```

**INSTRUCTIONS**:
1. Analyze the user's requirements → produce a conceptual plan:
   - For each task: name, purpose, dependencies, complexity.
   - If relevant to user_input_description, embed placeholders like {{{{title}}}} or {{{{targetLanguage}}}} in the tasks.
2. Determine agent types: role, goal, backstory.
   - If relevant, embed placeholders in those fields (e.g. "Translator for {{{{title}}}}").

Format your response as Python code that creates a ConceptualPlan object.
Start with "ConceptualPlan(" and include all TaskDefinition and AgentDefinition objects.

Example format:
```python
ConceptualPlan(
    planned_tasks=[
        TaskDefinition(
            name="TaskName",
            purpose="Task purpose",
            dependencies=[],
            complexity="Low"
        )
    ],
    required_agent_types=[
        AgentDefinition(
            name="AgentName",
            role="Agent role",
            goal="Agent goal",
            backstory="Agent backstory"
        )
    ]
)
```
"""
        return Task(
            description=description,
            expected_output="ConceptualPlan object with tasks and agent types",
            agent=self.planner_agent(),
            context=[self.gather_user_requirements_task()],
            output_pydantic=ConceptualPlan
        )

    @task
    def interpret_input_description_task(self) -> Task:
        """
        Takes 'inputDescription' from user requirements → partial input_schema_json snippet.
        E.g. if inputDescription says "Title to localize," produce:
        {
          "input_schema_json": {
            "title": {"type": "string", "description": "..."}
          }
        }
        """
        # Here we double any braces in the snippet.
        description = r"""
From user requirements (especially 'inputDescription'):
{{output}}

Your response should match the following Pydantic model:

```python
class InputSchemaDefinition(BaseModel):
    input_schema_json: Dict[str, Any] = {}
```

**INSTRUCTIONS**:
1. Construct partial `input_schema_json` from inputDescription
2. For each input field identified:
   - Set the appropriate type (string, number, boolean, etc.)
   - Add a clear description

Format your response as Python code that creates an InputSchemaDefinition object.
Start with "InputSchemaDefinition(input_schema_json=" and end with ")".

Example format:
```python
InputSchemaDefinition(input_schema_json={
    "title": {"type": "string", "description": "..."},
    "targetLanguage": {"type": "string", "description": "..."}
})
```
"""
        return Task(
            description=description,
            expected_output="InputSchemaDefinition with partial input_schema_json",
            agent=self.schema_converter(),
            context=[self.gather_user_requirements_task()],
            output_pydantic=InputSchemaDefinition
        )

    @task
    def generate_alternative_plans_task(self) -> Task:
        """
        Generates multiple alternative plans for Best-of-N sampling.
        """
        description = r"""
Based on the user requirements:
{{output}}

The constraints identified:
{constraints}

Generate {n_samples} alternative high-quality plans. Each plan should be different 
in approach, but all should meet the core requirements.

Your response should match the following Pydantic models:

```python
class TaskDefinition(BaseModel):
    name: str
    purpose: str
    dependencies: List[str] = []
    complexity: str = "Medium"  # Low, Medium, High

class AgentDefinition(BaseModel):
    name: str
    role: str
    goal: str
    backstory: str = ""

class Plan(BaseModel):
    name: str 
    content: str
    planned_tasks: List[TaskDefinition]
    required_agent_types: List[AgentDefinition]
    
class AlternativePlans(BaseModel):
    plans: List[Plan]
```

For each plan:
1. Give it a descriptive name
2. Provide a comprehensive description of the approach
3. Include the planned tasks and agents similar to the original plan format

Ensure each plan is meaningfully different in:
- Technical approach
- Team composition
- Process flow
- Resource allocation

Format your response as Python code that creates an AlternativePlans object with {n_samples} Plan objects.
Start with "AlternativePlans(plans=[" and include all Plan objects.
"""
        description = description.replace("{n_samples}", str(self.n_samples))
        description = description.replace("{constraints}", "{{constraints}}")
        
        return Task(
            description=description,
            expected_output=f"AlternativePlans object with {self.n_samples} alternative plans",
            agent=self.planner_agent(),
            context=[self.gather_user_requirements_task(), self.identify_constraints_task()],
            output_pydantic=AlternativePlans
        )
    
    @task
    def evaluate_plans_task(self) -> Task:
        """
        Evaluates multiple plans against constraints.
        """
        description = r"""
Evaluate these plans:
{{output}}

Against the identified constraints:
{constraints}

Your response should match the following Pydantic models:

```python
class PlanEvaluation(BaseModel):
    constraints_satisfied: List[str]
    constraints_violated: List[str]
    completeness: int  # 1-10
    efficiency: int  # 1-10
    feasibility: int  # 1-10
    alignment: int  # 1-10
    explanations: Dict[str, str]
    total_score: int

class EvaluatedPlan(BaseModel):
    name: str
    evaluation: PlanEvaluation

class PlanEvaluationResult(BaseModel):
    evaluated_plans: List[EvaluatedPlan]
    best_plan: str  # Name of highest scoring plan
```

For each plan:
1. Check if it satisfies each constraint
2. Rate it on a scale of 1-10 for:
   - Completeness
   - Efficiency
   - Feasibility
   - Alignment with requirements
3. Provide a brief explanation for each rating
4. Calculate total_score as the sum of all scores

Format your response as Python code that creates a PlanEvaluationResult object.
Start with "PlanEvaluationResult(" and include all EvaluatedPlan objects.
Be sure to identify which plan has the highest score and include that name in the 'best_plan' field.
"""
        description = description.replace("{constraints}", "{{constraints}}")
        
        return Task(
            description=description,
            expected_output="PlanEvaluationResult with evaluated plans and best plan",
            agent=self.plan_evaluator_agent(),
            context=[
                self.identify_constraints_task(),
                self.generate_alternative_plans_task() if self.use_best_of_n else self.plan_tasks_and_agents_task()
            ],
            output_pydantic=PlanEvaluationResult
        )

    @task
    def assemble_schema_task(self) -> Task:
        """
        Merges the conceptual plan + partial input_schema_json into a standard CrewAI schema.
        """
        # Double braces so .format won't interpret them. 
        description = r"""
We have a conceptual plan:
{{output}}

And also a partial input_schema_json from interpret_input_description_task.

Create a full CrewAI schema following this structure:

```python
class CrewConfig(BaseModel):
    crew: Dict[str, Any]
    agents: Any
    tasks: Any
    input_schema_json: Any
```

- Merge plan + input_schema_json
- Keep placeholders like {{{{title}}}} if they exist.

Format your response as Python code that creates a CrewConfig object.
Start with "CrewConfig(" and include all necessary fields.
"""
        return Task(
            description=description,
            expected_output="CrewConfig object with complete schema",
            agent=self.schema_converter(),
            context=[
                self.plan_tasks_and_agents_task(),
                self.interpret_input_description_task()
            ],
            output_pydantic=CrewConfig
        )

    @task
    def refine_and_output_final_config_task(self) -> Task:
        """
        Final step: ensures placeholders remain if relevant; cleans up extraneous text.
        """
        # Double braces around the snippet if you show example JSON.
        description = r"""
Given this draft config:
{{output}}

Your response should match the following Pydantic model:

```python
class CrewConfig(BaseModel):
    crew: Dict[str, Any]
    agents: Any
    tasks: Any
    input_schema_json: Any
```

**INSTRUCTIONS**:
1. Ensure "crew", "agents", "tasks", "input_schema_json" are present.
2. Each agent: name, role, goal, backstory. Keep placeholders {{{{title}}}} if relevant.
3. Each task: name, description, expected_output, agent, human_input, context_tasks.
   - Keep placeholders if they make sense. Remove truly extraneous placeholders only.

Format your response as Python code that creates a refined CrewConfig object.
Start with "CrewConfig(" and include all necessary fields.
"""
        return Task(
            description=description,
            expected_output="Refined CrewConfig object",
            agent=self.schema_converter(),
            context=[self.assemble_schema_task()],
            output_pydantic=CrewConfig
        )

    @crew
    def crew(self) -> Crew:
        """
        Final pipeline: gather -> constraints -> plan -> alternatives -> evaluate -> interpret -> assemble -> refine.
        Quadruple braces for placeholders, double braces for literal JSON. 
        """
        tasks = [
            self.gather_user_requirements_task(),
            self.identify_constraints_task(),
            self.plan_tasks_and_agents_task(),
        ]
        
        if self.use_best_of_n:
            tasks.extend([
                self.generate_alternative_plans_task(),
                self.evaluate_plans_task(),
            ])
            
        tasks.extend([
            self.interpret_input_description_task(),
            self.assemble_schema_task(),
            self.refine_and_output_final_config_task()
        ])
            
        return Crew(
            agents=[
                self.planner_agent(), 
                self.constraint_agent(), 
                self.plan_evaluator_agent(),
                self.schema_converter()
            ],
            tasks=tasks,
            process=Process.sequential,
            verbose=True
        )